# ExplainableSumm: Methods and tools for evaluating automated summarization with human feedback.
As natural language models advance, the training and evaluation of natural language models is limited by the metrics and data used for specific tasks. In the context of automatic summary generation, traditional metrics such as ROUGE and BLEU, among others, are commonly used, but these may not capture the true essence of summary quality. In this paper, we review the validity of such metrics in the current context, using an innovative OpenAI dataset composed of annotated summary comparisons with human feedback. It is observed that the automatic summaries often outperform the quality of human reference summaries, becoming almost indistinguishable from them. Through various experiments, we explore both the effectiveness of traditional evaluation metrics and the impact of certain features and characteristics on the perceived quality of a summary. This study offers three significant contributions: first, it provides a critical evaluation of standard metrics in the current context, highlighting the need for continuous adaptations. Second, it highlights the importance of human feedback and how it can enrich the evaluation process, providing valuable insights that traditional metrics may not capture. Finally, it introduces and validates innovative metrics and tools, such as those based on semantic similarity, and the ExplainSumm tool, which have proven effective in different contexts. It is hoped that this work will not only challenge conventional notions in the evaluation of automatic summaries, but also propose a route towards a more holistic and nuanced evaluation, capable of more reliably representing the quality and usefulness of summaries in the contemporary era.
