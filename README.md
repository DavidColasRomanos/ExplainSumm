# ExplainableSumm: Methods and tools for evaluating automated summarization with human feedback.
As natural language models advance, the training and evaluation of natural language models is limited by the metrics and data used for specific tasks. In the context of automatic summary generation, traditional metrics such as ROUGE and BLEU, among others, are commonly used, but these may not capture the true essence of summary quality. In this paper, we review the validity of such metrics in the current context, using an innovative OpenAI dataset composed of annotated summary comparisons with human feedback. It is observed that the automatic summaries often outperform the quality of human reference summaries, becoming almost indistinguishable from them. Through various experiments, we explore both the effectiveness of traditional evaluation metrics and the impact of certain features and characteristics on the perceived quality of a summary.
